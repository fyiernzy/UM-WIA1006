# Least Square Regression

Least squares regression is a type of linear regression where the algorithm learns to minimize the sum of squared errors between the predicted and observed values of the dependent variable. The error term Œµ is the difference between the observed and predicted values of the dependent variable. The error term Œµ has some assumptions that are required for the validity and efficiency of the least squares regression method. These assumptions are:

- The expected value of the error term is zero. This means that on average, the errors are unbiased and do not systematically overestimate or underestimate the true value of the dependent variable.

- The variance of the error term is constant for all values of ùë•. This means that the errors have a uniform spread and do not change with the level of the independent variable. This is also called homoscedasticity.

- The values of the error term are independent. This means that the errors are not correlated with each other and do not depend on previous or future values of the independent or dependent variables. This is also called no autocorrelation.

- The error term is normally distributed. This means that the errors follow a bell-shaped curve and have a mean of zero and a standard deviation of œÉ. This is also called normality.

